/*! \file main.cxx
 *  \brief Main program

    This code initialize MPI (if necessary) and reads parameters from the user input (see \ref ui.cxx)
    loads particle data from a N-body output (see \ref io.cxx) into an array of \ref NBody::Particle,
    analyzes the system, searches for structures then substructures (see \ref search.cxx),
    outputs the group ids (in two different fashions see \ref io.cxx) and can also analyze the structures
    (see \ref substructureproperties.cxx and \ref io.cxx).

    \todo remove aray mpi_idlist and mpi_indexlist as these arrays are unnecessary 
    \todo alter unbinding/sortbybindingenergy calls since seems a waste of cpu cycles
*/

#include "stf.h"

using namespace std;
using namespace Math;
using namespace NBody;

int main(int argc,char **argv)
{
#ifdef USEMPI
    //start MPI
    MPI_Init(&argc,&argv);
    //find out how big the SPMD world is 
    MPI_Comm_size(MPI_COMM_WORLD,&NProcs); 
    //and this processes' rank is 
    MPI_Comm_rank(MPI_COMM_WORLD,&ThisTask); 
#else
    int ThisTask=0,NProcs=1;
    Int_t Nlocal,Ntotal;
    Int_t Ntotalbaryon[3], Nlocalbaryon[3];
#endif
    Options opt;
    //get arguments
    GetArgs(argc, argv, opt);
    cout.precision(10);

    //variables
    //number of particles, (also number of baryons if use dm+baryon search) 
    //to store (point to) particle data
    Int_t nbodies,nbaryons; 
    Particle *Part;
    Particle *Pbaryons;
    KDTree *tree;

    //number in subset, number of grids used if iSingleHalo==0;
    Int_t nsubset, ngrid;
    //number of groups, temp num groups, and number of halos 
    Int_t ngroup, ng, nhalos;

    //to store group value (pfof), and also arrays to parse particles
    Int_t *pfof,*numingroup,**pglist;
    Int_t *pfofbaryons,*numingroupbaryons,**pglistbaryons;
    Int_t *pfofall;
    //to store information about the group
    PropData *pdata;

    //to store time and output time taken
    double time1,tottime;
    int nthreads;
#ifdef USEOPENMP
#pragma omp parallel
    {
    if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#else
    nthreads=1;
#endif
    tottime=MyGetTime();

    Coordinate cm,cmvel;
    Double_t Mtot;
    char fname1[1000],fname2[1000],fname3[1000],fname4[1000];

#ifdef USEMPI
    mpi_nlocal=new Int_t[NProcs];
    mpi_domain=new MPI_Domain[NProcs];
    mpi_nsend=new Int_t[NProcs*NProcs];
    mpi_ngroups=new Int_t[NProcs];
    //store MinSize as when using mpi prior to stitching use min of 2;
    MinNumMPI=2;
    //if single halo, use minsize to initialize the old minimum number
    //else use the halominsize since if mpi and not single halo, halos localized to mpi domain for substructure search
    if (opt.iSingleHalo) MinNumOld=opt.MinSize;
    else MinNumOld=opt.HaloMinSize;
#endif

    //read particle information and allocate memory
    time1=MyGetTime();
    //for MPI determine total number of particles AND the number of particles assigned to each processor
#ifdef USEMPI
    if (ThisTask==0) {
#endif
    cout<<"Read header ... "<<endl;
    nbodies=ReadHeader(opt);
    ///\todo need to update for MPI
    if (opt.igsflag==1) {
        nbaryons=0;
        int pstemp=opt.partsearchtype;
        opt.partsearchtype=PSTGAS;
        nbaryons+=ReadHeader(opt);
        opt.partsearchtype=PSTSTAR;
        nbaryons+=ReadHeader(opt);
        opt.partsearchtype=PSTBH;
        nbaryons+=ReadHeader(opt);
        opt.partsearchtype=pstemp;
    }
    else nbaryons=0;
#ifdef USEMPI
    }
    MPI_Bcast(&nbodies,1, MPI_Int_t,0,MPI_COMM_WORLD);
    //initial estimate need for memory allocation assuming that work balance is not greatly off
    if (ThisTask==0)
#endif
    cout<<"There are "<<nbodies<<" particles"<<endl;

#ifndef USEMPI
    Nlocal=nbodies;
    if (opt.igsflag==1) {
        cout<<"There are "<<nbaryons<<" baryon particles"<<endl;
        Part=new Particle[nbodies+nbaryons];
        Pbaryons=&Part[nbodies];
        Nlocalbaryon[0]=nbaryons;
    }
    else {
        Part=new Particle[nbodies];
        Pbaryons=NULL;
        nbaryons=0;
    }
#else
    Nlocal=nbodies/NProcs*MPIProcFac;
    NExport=Nlocal*MPIExportFac;
#ifdef MPIREDUCEMEM
    MPINumInDomain(opt);
    if (NProcs==1) {Nlocal=Nmemlocal=nbodies;NExport=NImport=1;}
    cout<<ThisTask<<" allocating enough memory for "<<Nlocal<<endl;
#endif
#ifdef MPIREDUCEMEM
        Part=new Particle[Nmemlocal];
        mpi_idlist=new Int_t[Nmemlocal];
        mpi_indexlist=new Int_t[Nmemlocal];
#else
        Part=new Particle[Nlocal];
        mpi_idlist=new Int_t[Nlocal];
        mpi_indexlist=new Int_t[Nlocal];
#endif
    if (opt.igsflag==1) {
#ifdef MPIREDUCEMEM
        Part=new Particle[Nmemlocal+Nmemlocalbaryon];
        mpi_idlist=new Int_t[Nmemlocal+Nmemlocalbaryon];
        mpi_indexlist=new Int_t[Nmemlocal+Nmemlocalbaryon];
        Pbaryons=&Part[Nmemlocal];
#else
        Part=new Particle[Nlocal+Nlocalbaryon];
        mpi_idlist=new Int_t[Nlocal+Nlocalbaryon];
        mpi_indexlist=new Int_t[Nlocal+Nlocalbaryon];
        Pbaryons=&Part[Nlocal];
#endif
        nbaryons=Nlocalbaryon[0];
        cout<<"There are "<<nbaryons<<" baryon particles"<<endl;
    }
    else {
#ifdef MPIREDUCEMEM
        Part=new Particle[Nmemlocal];
        mpi_idlist=new Int_t[Nmemlocal];
        mpi_indexlist=new Int_t[Nmemlocal];
#else
        Part=new Particle[Nlocal];
        mpi_idlist=new Int_t[Nlocal];
        mpi_indexlist=new Int_t[Nlocal];
#endif
        Pbaryons=NULL;
        nbaryons=0;
    }
    //now allocate memory for either all particles or in the case of MPI, each processor allocates memory for Nslab particles
#endif

    //now read particle data
#ifdef USEMPI
    if (ThisTask==0) 
#endif
    cout<<"Loading ... "<<endl;
    ReadData(opt, Part, nbodies, Pbaryons, nbaryons);
    //store ids separate from particle ids as particle ids used to ensure memory can be addressed in original particle order no matter how this order is changed. 

#ifdef USEMPI
    if (ThisTask==0) 
#endif
    cout<<"Done Loading"<<endl;
    time1=MyGetTime()-time1;
    cout<<"TIME::"<<ThisTask<<" took "<<time1<<" to load "<<Ntotal<<" "<<Nlocal<<endl;
#ifdef USEMPI
    Ntotal=nbodies;
    nbodies=Nlocal;
    NExport=Nlocal*MPIExportFac;
    mpi_period=opt.p;
    MPI_Allgather(&nbodies, 1, MPI_Int_t, mpi_nlocal, 1, MPI_Int_t, MPI_COMM_WORLD);
#endif

    //set filenames if they have been passed
#ifdef USEMPI
    if (opt.smname!=NULL) sprintf(fname4,"%s.%d",opt.smname,ThisTask);
#else
    if (opt.smname!=NULL) sprintf(fname4,"%s",opt.smname);
#endif

    //read local velocity data or calculate it 
    //(and if STRUCDEN flag or HALOONLYDEN is set then only calculate the velocity density function for objects within a structure
    //as found by SearchFullSet)
#if defined (STRUCDEN) || defined (HALOONLYDEN)
#else

    time1=MyGetTime();
    if(FileExists(fname4)) ReadLocalVelocityDensity(opt, nbodies,Part);
    else{
        GetVelocityDensity(opt, nbodies, Part);
        WriteLocalVelocityDensity(opt, nbodies,Part);
    }
    time1=MyGetTime()-time1;
    cout<<"TIME::"<<ThisTask<<" took "<<time1<<" to analyze "<<Nlocal<<" with "<<nthreads<<endl;
#endif

    //here adjust Efrac to Omega_cdm/Omega_m from what it was before if baryonic search is separate
    if (opt.igsflag && opt.partsearchtype!=PSTALL) opt.uinfo.Eratio*=opt.Omega_cdm/opt.Omega_m;

    //From here can either search entire particle array for "Halos" or if a single halo is loaded, then can just search for substructure
    if (!opt.iSingleHalo) {
#ifndef USEMPI
        time1=MyGetTime();
        pfof=SearchFullSet(opt,nbodies,Part,ngroup);
        nhalos=ngroup;
        cout<<"TIME:: took "<<time1<<" to search "<<nbodies<<" with "<<nthreads<<endl;
#else
        //nbodies=Ntotal;
        ///\todo Communication Buffer size determination and allocation. For example, eventually need something like FoFDataIn = (struct fofdata_in *) CommBuffer;
        ///At the moment just using NExport
        NExport=Nlocal*MPIExportFac;
        mpi_foftask=new Int_t[Nlocal];
        MPISetTaskID(Nlocal);
        //Now when MPI invoked this returns pfof after local linking and linking across and also reorders groups
        //according to size and localizes the particles belong to the same group to the same mpi thread.
        //after this is called Nlocal is adjusted to the local subset where groups are localized to a given mpi thread.
        time1=MyGetTime();
        pfof=SearchFullSet(opt,Nlocal,Part,ngroup);
        time1=MyGetTime()-time1;
        cout<<"TIME::"<<ThisTask<<" took "<<time1<<" to search "<<Nlocal<<" with "<<nthreads<<endl;
        nbodies=Nlocal;
#ifdef MPIREDUCEMEM
        //if memory was allocated because initial storage was not enough then assign
        if (Nlocal>Nmemlocal)
#endif
        Part=PartDataGet;
        nhalos=ngroup;
        //place barrier here to ensure all mpi threads have pfof for groups localized to their memory
        MPI_Barrier(MPI_COMM_WORLD);
#endif
    }
    else {
        Coordinate *gvel;
        Matrix *gveldisp;
        GridCell *grid;
        ///\todo Scaling is still not MPI compatible
#ifdef SCALING
        ScaleLinkingLengths(opt,nbodies,Part,cm,cmvel,Mtot);
#endif
        opt.Ncell=opt.Ncellfac*nbodies;
        //build grid using leaf nodes of tree (which is guaranteed to be adaptive and have maximum number of particles in cell of tree bucket size)
        tree=InitializeTreeGrid(opt,nbodies,Part);
        ngrid=tree->GetNumLeafNodes();
        cout<<"Given "<<nbodies<<" particles, and max cell size of "<<opt.Ncell<<" there are "<<ngrid<<" leaf nodes or grid cells, with each node containing ~"<<nbodies/ngrid<<" particles"<<endl;
        grid=new GridCell[ngrid];
        //note that after this system is back in original order as tree has been deleted.
        FillTreeGrid(opt, nbodies, ngrid, tree, Part, grid);
        //calculate cell quantities to get mean field
        gvel=GetCellVel(opt,nbodies,Part,ngrid,grid);
        gveldisp=GetCellVelDisp(opt,nbodies,Part,ngrid,grid,gvel);
        opt.HaloSigmaV=0;for (int j=0;j<ngrid;j++) opt.HaloSigmaV+=pow(gveldisp[j].Det(),1./3.);opt.HaloSigmaV/=(double)ngrid;

        //now that have the grid cell volume quantities and local volume density 
        //can determine the logarithmic ratio between the particle velocity density and that predicted by the background velocity distribution
        GetDenVRatio(opt,nbodies,Part,ngrid,grid,gvel,gveldisp);
        //WriteDenVRatio(opt,nbodies,Part);
        //and then determine how much of an outlier it is
        nsubset=GetOutliersValues(opt,nbodies,Part);
        //save the normalized denvratio and also determine how many particles lie above the threshold.
        //nsubset=WriteOutlierValues(opt, nbodies,Part);
        //Now check if any particles are above the threshold
        if (nsubset==0) {
            cout<<"no particles found above threshold of "<<opt.ellthreshold<<endl;
            cout<<"Exiting"<<endl;
            return 0;
        }
        else cout<<nsubset<< " above threshold of "<<opt.ellthreshold<<" to be searched"<<endl;
#ifndef USEMPI
        pfof=SearchSubset(opt,nbodies,nbodies,Part,ngroup);
#else
        //nbodies=Ntotal;
        ///\todo Communication Buffer size determination and allocation. For example, eventually need something like FoFDataIn = (struct fofdata_in *) CommBuffer;
        ///At the moment just using NExport
        NExport=Nlocal*MPIExportFac;
        mpi_foftask=new Int_t[Nlocal];
        MPISetTaskID(nbodies);

        //Now when MPI invoked this returns pfof after local linking and linking across and also reorders groups 
        //according to size and localizes the particles belong to the same group to the same mpi thread.
        //after this is called Nlocal is adjusted to the local subset where groups are localized to a given mpi thread.
        pfof=SearchSubset(opt,Nlocal,Nlocal,Part,ngroup);
        nbodies=Nlocal;
        Part=PartDataGet;
        //place barrier here to ensure all mpi threads have pfof for groups localized to their memory
        MPI_Barrier(MPI_COMM_WORLD);
#endif
    }
    cout<<"Searching subset"<<endl;
    time1=MyGetTime();
    //if groups have been found (and localized to single MPI thread) then proceed to search for subsubstructures
    SearchSubSub(opt, nbodies, Part, pfof,ngroup,nhalos);
    time1=MyGetTime()-time1;
    cout<<"TIME::"<<ThisTask<<" took "<<time1<<" to search for substructures "<<Nlocal<<" with "<<nthreads<<endl;

    //if only searching initially for dark matter groups, once found, search for associated baryonic structures if requried
    if (opt.igsflag) {
        time1=MyGetTime();
        pfofall=SearchBaryons(opt, nbaryons, Pbaryons, nbodies, Part, pfof, ngroup,nhalos,opt.iseparatefiles);
        pfofbaryons=&pfofall[nbodies];
        numingroupbaryons=BuildNumInGroup(nbaryons, ngroup, pfofbaryons);
        pglistbaryons=new Int_t*[ngroup+1];
        for (Int_t i=1;i<=ngroup;i++) {
            pglistbaryons[i]=new Int_t[numingroupbaryons[i]+1];
            pglistbaryons[i][numingroupbaryons[i]]=numingroupbaryons[i];
            numingroupbaryons[i]=0;
        }
        for (Int_t i=0;i<nbaryons;i++) {
            if (pfofbaryons[i]>0) pglistbaryons[pfofbaryons[i]][numingroupbaryons[pfofbaryons[i]]]=i;
            numingroupbaryons[pfofbaryons[i]]++;
        }
        time1=MyGetTime()-time1;
        cout<<"TIME::"<<ThisTask<<" took "<<time1<<" to search baryons  with "<<nthreads<<endl;
    }

    //get mpi local hierarchy
    Int_t *nsub,*parentgid, *uparentgid,*stype;
    nsub=new Int_t[ngroup+1];
    parentgid=new Int_t[ngroup+1];
    uparentgid=new Int_t[ngroup+1];
    stype=new Int_t[ngroup+1];
    pdata=new PropData[ngroup+1];
    Int_t nhierarchy=GetHierarchy(opt,ngroup,nsub,parentgid,uparentgid,stype);
    CopyHierarchy(opt,pdata,ngroup,nsub,parentgid,uparentgid,stype);
    
    //if a separate baryon search has been run, now just place all particles together if seperate files for halos, subhalos, and baryons is NOT requested
    if (opt.igsflag) {
        delete[] pfof;
        pfof=&pfofall[0];
        //if (opt.iseparatefiles==0){
        nbodies+=nbaryons;
        //opt.igsflag=0;
        Nlocal=nbodies;
        //}
    }

    //output results
    if(opt.iwritefof) {
#ifdef USEMPI
        if (ThisTask==0) {
            mpi_pfof=new Int_t[Ntotal];
            //since pfof is a local subset, not all pfof values have been set, thus initialize them to zero.
            for (Int_t i=0;i<Ntotal;i++) mpi_pfof[i]=0;
        }
        MPICollectFOF(Ntotal, pfof);
        if (ThisTask==0) WriteFOF(opt,Ntotal,mpi_pfof);
#else
        WriteFOF(opt,nbodies,pfof);
#endif
    }

        numingroup=BuildNumInGroup(Nlocal, ngroup, pfof);
    //if separate files explicitly save halos, associated baryons, and subhalos separately
    if (opt.iseparatefiles) {
        pglist=SortAccordingtoBindingEnergy(opt,Nlocal,Part,nhalos,pfof,numingroup,pdata);//alters pglist so most bound particles first
        WriteProperties(opt,nhalos,pdata);
        WriteGroupCatalog(opt, nhalos, numingroup, pglist, Part,ngroup-nhalos);
        //if baryons have been searched output related gas baryon catalogue
        if (opt.igsflag || opt.partsearchtype==PSTALL){
            //sprintf(fname2,"%s",opt.outname);
            //sprintf(fname1,"%s.baryons",opt.outname);
            //sprintf(opt.outname,"%s",fname1);
            //WriteGroupCatalog(opt, nhalos, numingroupbaryons, pglistbaryons, Pbaryons);
            //for (Int_t i=1;i<=nhalos;i++) if (numingroupbaryons[i]>0) delete[] pglistbaryons[i];
            //sprintf(opt.outname,"%s",fname2);
            WriteGroupPartType(opt, nhalos, numingroup, pglist, Part);
        }
        for (Int_t i=1;i<=nhalos;i++) delete[] pglist[i];
        delete[] pglist;
    }
    WriteHierarchy(opt,ngroup,nhierarchy,psldata->nsinlevel,nsub,parentgid,stype);
    Int_t indexii=0;
    ng=ngroup;
    //if separate files, alter offsets
    if (opt.iseparatefiles) {
        sprintf(fname1,"%s.sublevels",opt.outname);
        sprintf(opt.outname,"%s",fname1);
        //alter index point to just output sublevels (assumes no reordering and assumes no change in nhalos as a result of unbinding in SubSubSearch)
        indexii=nhalos;
        ng=ngroup-nhalos;
    }

    if (ng>0) {
        pglist=SortAccordingtoBindingEnergy(opt,nbodies,Part,ng,pfof,&numingroup[indexii],&pdata[indexii],indexii);//alters pglist so most bound particles first
        WriteProperties(opt,ng,&pdata[indexii]);
        WriteGroupCatalog(opt, ng, &numingroup[indexii], pglist, Part);
        WriteHierarchy(opt,ngroup,nhierarchy,psldata->nsinlevel,nsub,parentgid,stype,1);
        if (opt.igsflag || opt.partsearchtype==PSTALL){
            //sprintf(fname2,"%s",opt.outname);
            //sprintf(fname1,"%s.baryons",opt.outname);
            //sprintf(opt.outname,"%s",fname1);
            //WriteGroupCatalog(opt, ng, &numingroupbaryons[indexii], &pglistbaryons[indexii], Pbaryons);
            //for (Int_t i=1;i<=ng;i++) if (numingroupbaryons[i+indexii]>0) delete[] pglistbaryons[i+indexii];
            //delete[] pglistbaryons;
            //delete[] numingroupbaryons;
            //sprintf(opt.outname,"%s",fname2);
            WriteGroupPartType(opt, ng, &numingroup[indexii], pglist, Part);
        }
        for (Int_t i=1;i<=ng;i++) delete[] pglist[i];
        delete[] pglist;
    }
    else {
        WriteGroupCatalog(opt,ng,&numingroup[indexii],NULL,Part);
        WriteHierarchy(opt,ngroup,nhierarchy,psldata->nsinlevel,nsub,parentgid,stype,1);
        if (opt.igsflag){
            WriteGroupPartType(opt, ng, &numingroup[indexii], NULL, Part);
        }
    }
    delete[] numingroup;
    delete[] pdata;

    tottime=MyGetTime()-tottime;
    cout<<"TIME::"<<ThisTask<<" took "<<tottime<<" in all"<<endl;

#ifdef USEMPI
    MPI_Finalize();
#endif

    return 0;
}

